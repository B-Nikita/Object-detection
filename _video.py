# -*- coding: utf-8 -*-
"""_video.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KL6C-wbjwsiLLfjh4QdbBxC22sfAtu9N
"""

from __future__ import division
import time
import torch 
import torch.nn as nn
from torch.autograd import Variable
import numpy as np
import cv2 
from util import *
import argparse
import os 
import os.path as osp
from darknet import Darknet
import pickle as pkl
import pandas as pd
import random

import glob
from sys import platform
from IPython.display import HTML
from base64 import b64encode

def arg_parse():
  """
  Parse arguements to the detect module
    
  """
    
  parser = argparse.ArgumentParser(description='YOLO v3 Real time Object Detection Module')
  parser.add_argument("--bs", dest = "bs", help = "Batch size", default = 1)
  parser.add_argument("--confidence", dest = "confidence", help = "Object Confidence to filter predictions", default = 0.5)
  parser.add_argument('--names', type=str, default='data/coco.names', help='*.names path')
  parser.add_argument("--nms_thresh", dest = "nms_thresh", help = "IOU Threshhold for NMS", default = 0.4)
  parser.add_argument("--cfg", dest = 'cfgfile', help = 
                        "Config file",
                        default = "yolov3.cfg", type = str)
  parser.add_argument("--weights", dest = 'weightsfile', help = 
                        "weightsfile",
                        default = "yolov3.weights", type = str)
  parser.add_argument("--reso", dest = 'reso', help = 
                        "Input resolution of the network. Increase to increase accuracy. Decrease to increase speed",
                        default = "416", type = str)
  parser.add_argument("--video", dest = "videofile", help = "Video file to run detection on", default = "Visit_LA.mp4", type = str)
  parser.add_argument('--device', default='', help='device id (i.e. 0 or 0,1) or cpu')
  parser.add_argument('--img-size', type=int, default=416, help='inference size (pixels)')
  parser.add_argument('--classes', nargs='+', type=int, help='filter by class')
  parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')

  return parser.parse_args()

  


args = arg_parse()
batch_size = int(args.bs)
confidence = float(args.confidence)
nms_thresh = float(args.nms_thresh)

start = 0
CUDA = torch.cuda.is_available()



num_classes = 80
classes = load_classes("data/coco.names")

#Set up the neural network
print("Loading network.....")
model = Darknet(args.cfgfile)
model.load_weights(args.weightsfile)
print("Network successfully loaded")

model.net_info["height"] = args.reso
inp_dim = int(model.net_info["height"])
assert inp_dim % 32 == 0 
assert inp_dim > 32

#If there's a GPU availible, put the model on GPU
'''if CUDA:
    model.cuda()'''


#Set the model in evaluation mode
model.eval()

def write(x, results):
  c1 = tuple(x[1:3].int())
  c2 = tuple(x[3:5].int())
  img = results
  cls = int(x[-1])
  colors = pkl.load(open("pallete", "rb"))
  color = random.choice(colors)
  label = "{0}".format(classes[cls])
  cv2.rectangle(img, c1, c2,color, 1)
  t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]
  c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4
  cv2.rectangle(img, c1, c2,color, -1)
  cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);
  return img

#Detection phase

#First open the video in openCV
def predict_video(path_video=args.videofile,output_dir='output'):
  if not os.path.exists(output_dir):
    os.makedirs(output_dir)

  cap = cv2.VideoCapture(path_video)
  assert cap.isOpened(), 'Cannot capture source'

  frames = 0  
  start = time.time()
  while cap.isOpened(): #Iterating over the frames in a similar mannner to the way we iterated over images
    ret, frame = cap.read()
    save_path = os.path.join(output_dir, os.path.split(path_video)[-1]) 
    fps = cap.get(cv2.CAP_PROP_FPS)
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'MP4V'), fps, (w, h))

    
    if ret:   
      img = prep_image(frame, inp_dim)
      #cv2.imshow("a", frame)
      im_dim = frame.shape[1], frame.shape[0]
      im_dim = torch.FloatTensor(im_dim).repeat(1,2)   
                      
      if CUDA:
        im_dim = im_dim.cuda()
        img = img.cuda()
          
      with torch.no_grad():
        output = model(Variable(img, volatile = True), CUDA)

      output = write_results(output, confidence, num_classes, nms_conf = nms_thresh)


      if type(output) == int:
        frames += 1 #Keeping a track of the number of frames captured
        #Devide this(frames) by the time elapsed since the first frame to print the FPS of the video
        print("FPS of the video is {:5.4f}".format( frames / (time.time() - start)))
        #cv2.imshow("frame", frame) #To display the frame with bounding box
        vid_writer.write(frame)
        key = cv2.waitKey(1)

        #If user presses the Q key, it causes the code to break the loop, and the video ends
        if key & 0xFF == ord('q'): 
          break
        continue

      im_dim = im_dim.repeat(output.size(0), 1)
      scaling_factor = torch.min(416/im_dim,1)[0].view(-1,1)
          
      output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2
      output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2
          
      output[:,1:5] /= scaling_factor

      for i in range(output.shape[0]):
        output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])
        output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])
      
      classes = load_classes('data/coco.names')
      colors = pkl.load(open("pallete", "rb"))

      list(map(lambda x: write(x, frame), output))
          
      #cv2.imshow("frame", frame)
      vid_writer.write(frame)
      key = cv2.waitKey(1)
      if key & 0xFF == ord('q'):
        break
      frames += 1
      print(time.time() - start)
      print("FPS of the video is {:5.2f}".format( frames / (time.time() - start)))
    else:
      vid_writer.release()
      break

  return save_path


path_video = os.path.join("input_video","Visit_LA.mp4")
save_path = predict_video(path_video)

# compress video
compressed_path = os.path.join("output_compressed", os.path.split(save_path)[-1])
os.system(f"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}")

# Show video
mp4 = open(compressed_path,'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""
<video width=400 controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)

