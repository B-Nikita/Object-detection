# -*- coding: utf-8 -*-
"""darknet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hqqHcvOwxmLQyKBFGCY4c-_zsjjg1BAp
"""

from __future__ import division

import torch
import cv2
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from util import *
from torch.autograd import Variable

"""##### def get_test_input()"""

'''
img = cv2.imread("imgs/dog.jpg")
cv2_imshow(img)
'''
def get_test_input():
  img = cv2.imread("imgs/dog.jpg")
  #print(img)  
  img = cv2.resize(img, (416,416))          #Resize to the input dimension
  img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W 
  img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise
  img_ = torch.from_numpy(img_).float()     #Convert to float
  img_ = Variable(img_)                     # Convert to Variable
  return img_

"""##### def parse_cfg(cfgfile)

The idea here is to parse the cfg, and **store every block as a dict**. The attributes of the blocks and their values are stored as key-value pairs in the dictionary. As we parse through the cfg, we keep appending these dicts, denoted by the variable `block` in our code, to a list blocks. Our function will return this block.
"""

def parse_cfg(cfgfile):
  '''
  Takes a configuration file
    
  Returns a list of blocks. Each blocks describes a block in the neural
  network to be built. Block is represented as a dictionary in the list
  '''
  file = open(cfgfile, 'r')
  lines = file.read().split('\n')                        # store the lines in a list
  lines = [x for x in lines if len(x) > 0]               # get rid of the empty lines 
  lines = [x for x in lines if x[0] != '#']              # get rid of comments
  lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces

  block = {}
  blocks = []

  for line in lines:
      if line[0] == "[":               # This marks the start of a new block
          if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.
              blocks.append(block)     # add it the blocks list
              block = {}               # re-init the block
          block["type"] = line[1:-1].rstrip()     
      else:
          key,value = line.split("=") 
          block[key.rstrip()] = value.lstrip()
  blocks.append(block)

  return blocks

"""### Empty Layer
Now, an empty layer might seem weird given it does nothing. The Route Layer, just like any other layer performs an operation (bringing forward previous layer / concatenation). In PyTorch, when we define a new layer, we subclass `nn.Module` and write the operation the layer performs in the `forward` function of the `nn.Module` object.

For designing a layer for the Route block, we will have to build a nn.Module object that is initialized with values of the attribute layers as it's member(s). Then, we can write the code to concatenate/bring forward the feature maps in the forward function. Finally, we then execute this layer in th forward function of our network.

But given the code of concatenation is fairly short and simple (calling torch.cat on feature maps), designing a layer as above will lead to unnecessary abstraction that just increases boiler plate code. Instead, what we can do is put a dummy layer in place of a proposed route layer, and then perform the concatenation directly in the forward function of the nn.Module object representing darknet. 

The convolutional layer just in front of a route layer applies it's kernel to (possibly concatenated) feature maps from a previous layers. The following code updates the `filters` variable to hold the number of filters outputted by a route layer.
"""

class EmptyLayer(nn.Module):
  def __init__(self):
    super(EmptyLayer, self).__init__()

"""### Detection Layer
We define a new layer DetectionLayer that holds the anchors used to detect bounding boxes.
"""

class DetectionLayer(nn.Module):
  def __init__(self, anchors):
    super(DetectionLayer, self).__init__()
    self.anchors = anchors

"""We have 5 types of layers in the list (mentioned above). PyTorch provides pre-built layers for types `convolutional` and `upsample`. We will have to write our own modules for the rest of the layers by extending the `nn.Module` class."""

def create_modules(blocks):
  net_info = blocks[0]     #Captures the information about the input and pre-processing    
  module_list = nn.ModuleList()
  prev_filters = 3
  output_filters = []
    
  for index, x in enumerate(blocks[1:]):
    module = nn.Sequential()
    #check the type of block
    #create a new module for the block
    #append to module_list
        
    #If it's a convolutional layer
    if (x["type"] == "convolutional"):
      #Get the info about the layer
      activation = x["activation"]
      activation = x["activation"]
      try:
        batch_normalize=int(x["batch_normalize"])
        bias=False
      except:
        batch_normalize=0
        bias=True

      filters= int(x["filters"])
      padding = int(x["pad"])
      kernel_size = int(x["size"])
      stride = int(x["stride"])
      
      if padding:
        pad = (kernel_size - 1) // 2
      else:
        pad = 0

      #Add the convolutional layer
      conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)
      module.add_module("conv_{0}".format(index), conv)

      #Add the Batch Norm Layer
      if batch_normalize:
        bn = nn.BatchNorm2d(filters)
        module.add_module("batch_norm_{0}".format(index), bn)
           
      #Check the activation. 
      #It is either Linear or a Leaky ReLU for YOLO
      if activation == "leaky":
        activn = nn.LeakyReLU(0.1, inplace = True)
        module.add_module("leaky_{0}".format(index), activn)

      #If it's an upsampling layer
      #We use Bilinear2dUpsampling
    elif (x["type"] == "upsample"):
      stride = int(x["stride"])
      upsample = nn.Upsample(scale_factor = 2, mode = "bilinear")
      module.add_module("upsample_{}".format(index), upsample)

    #If it is a route layer
    elif (x["type"] == "route"):
      x["layers"] = x["layers"].split(',')
      #Start  of a route
      start = int(x["layers"][0])
      #end, if there exists one.
      try:
        end = int(x["layers"][1])
      except:
        end = 0
      
      #Positive anotation
      if start > 0: 
        start = start - index
      if end > 0:
        end = end - index
      route = EmptyLayer()
      module.add_module("route_{0}".format(index), route)
      if end < 0:
        filters = output_filters[index + start] + output_filters[index + end]
      else:
        filters= output_filters[index + start]

    #shortcut corresponds to skip connection
    elif x["type"] == "shortcut":
       shortcut = EmptyLayer()
       module.add_module("shortcut_{}".format(index), shortcut)

    #Yolo is the detection layer
    elif x["type"] == "yolo":
      mask = x["mask"].split(",")
      mask = [int(x) for x in mask]
    
      anchors = x["anchors"].split(",")
      anchors = [int(a) for a in anchors]
      anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]
      anchors = [anchors[i] for i in mask]
    
      detection = DetectionLayer(anchors)
      module.add_module("Detection_{}".format(index), detection)
                              
    module_list.append(module)
    prev_filters = filters
    output_filters.append(filters)

  return (net_info, module_list)

"""**nn.ModuleList**
Our function will return a `nn.ModuleList`. This class is almost like a normal list containing `nn.Module` objects. However, when we add nn.ModuleList as a member of a nn.Module object (i.e. when we add modules to our network), all the parameters of nn.Module objects (modules) inside the nn.ModuleList are added as parameters of the nn.Module object (i.e. our network, which we are adding the nn.ModuleList as a member of) as well.

When we define a new convolutional layer, we must define the dimension of it's kernel. While the height and width of kernel is provided by the cfg file, the depth of the kernel is precisely the number of filters (or depth of the feature map) present in the previous layer. This means we need to **keep track of number of filters in the layer on which the convolutional layer is being applied**. We use the variable prev_filter to do this. We initialise this to 3, as the image has 3 filters corresponding to the RGB channels.

`nn.Sequential` class is used to sequentially execute a number of nn.Module objects. If you look at the cfg, you will realize a block may contain more than one layer. For example, a block of type convolutional has a batch norm layer as well as leaky ReLU activation layer in addition to a convolutional layer. We string together these layers using the nn.Sequential and it's the add_module function. For example, this is how we create the convolutional and the upsample layers.

#### Shortcut layer
The shortcut layer also makes use of an empty layer, for it also performs a very simple operation (addition). There is no need to update update the filters variable as it merely adds a feature maps of a previous layer to those of layer just behind.

### Testing the code for layer creation
"""

blocks = parse_cfg("yolov3.cfg")
print(create_modules(blocks))

"""### Defining the Network (Imlementing the forward pass of the network)

`forward` takes three arguments, self, the input x and CUDA, which if true, would use GPU to accelerate the forward pass.

Here, we iterate over `self.blocks[1:]` instead of `self.blocks` since the first element of self.blocks is a net block which isn't a part of the forward pass.

Since *route* and *shortcut* layers need output maps from previous layers, we cache the output feature maps of every layer in a dict outputs. The keys are the the indices of the layers, and the values are the feature maps

As was the case with `create_modules` function, we now iterate over `module_list` which contains the modules of the network. The thing to notice here is that the modules have been appended in the same order as they are present in the configuration file. This means, we can simply run our input through each module to get our output.
"""

class Darknet(nn.Module):
  def __init__(self,cfgfile):
    super(Darknet,self).__init__()
    self.blocks=parse_cfg(cfgfile)
    self.net_info,self.module_list = create_modules(self.blocks)

#forward takes three arguments, self, the input x and CUDA, which if true,
# would use GPU to accelerate the forward pass.
  def forward(self, x, CUDA):
    modules = self.blocks[1:]
    outputs= {} #We cache the outputs for the route layer

    write=0
    #Iterating over module_list which contains modules of the network
    for i,module in enumerate(modules):
      module_type=(module['type'])
      
      #Convolutional and Upsample layers
      if module_type == 'convolutional' or module_type=='upsample':
        x=self.module_list[i](x)

      #Route layer / Shortcut layer
      elif module_type == 'route':
        layers = module['layers']
        layers = [int(a) for a in layers]

        if(layers[0]) > 0:
          layers[0] = layers[0] - i

        if len(layers) == 1:
          x = outputs[i+ (layers[0])]

        else:
          if (layers[1]) > 0:
            layers[1] = layers[1] - i

          map1 = outputs[i + layers[0]]
          map2 = outputs[i + layers[1]]
          #Concatenating two featuremaps , 1 represents channel dimension
          x = torch.cat((map1,map2), 1)

      elif module_type == 'shortcut':
        from_=int(module['from'])
        x=outputs[i-1] + outputs[i+from_]

      elif module_type == 'yolo':
        anchors = self.module_list[i][0].anchors
        #print('anchors: ',anchors)

        #Get the input dimensions
        inp_dim=int(self.net_info['height'])
        #print('inp_dim: ',inp_dim)

        #Get the number of classes
        num_classes = int(module['classes'])
        #print('Number of classes: ',num_classes)

        #Transform
        x=x.data
        #print(x)
        x=predict_transform(x,inp_dim,anchors,num_classes, CUDA)
        if not write:     #if no collector has been initialised
          detections = x
          write= 1

        else:
          detections = torch.cat((detections,x),1)

      outputs[i] = x

    return detections
  
  def load_weights(self,weightfile):
    #open the weights file
    fp=open(weightfile,'rb')

    #The first 5 values are header information
    # 1. Major version number
    # 2. Minor version number
    # 3. Subversion number
    # 4,5. Images seen by the network(during training)
    header=np.fromfile(fp,dtype=np.int32, count=5) 
    self.header=torch.from_numpy(header)
    self.seen=self.header[3] 

    #rest of bits now represent the weights
    weights=np.fromfile(fp, dtype = np.float32)

    #Now we iterate over the weights file, and load the weights
    #into modules of our network
    ptr=0   #To keep track of where we are in the weights array
    for i in range(len(self.module_list)):
      module_type = self.blocks[i + 1]['type']

      #if module_type is convolutional load weights otherwise ignore
      if module_type == 'convolutional':
        model = self.module_list[i]
        try:
          batch_normalize = int(self.blocks[i+1]['batch_normalize'])
        except:
          batch_normalize = 0

        conv = model[0]

        #if batch_normalize is true, we load the weights as follows.
        if (batch_normalize):
          bn = model[1]

          #Get the number of weights of batch norm layer
          num_bn_biases = bn.bias.numel()

          #Load the weights
          bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])
          ptr += num_bn_biases

          bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
          ptr  += num_bn_biases

          bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
          ptr  += num_bn_biases
        
          bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])
          ptr  += num_bn_biases
        
          #Cast the loaded weights into dims of model weights.
          bn_biases = bn_biases.view_as(bn.bias.data)
          bn_weights = bn_weights.view_as(bn.weight.data)
          bn_running_mean = bn_running_mean.view_as(bn.running_mean)
          bn_running_var = bn_running_var.view_as(bn.running_var)

          #Copy the data to model
          bn.bias.data.copy_(bn_biases)
          bn.weight.data.copy_(bn_weights)
          bn.running_mean.copy_(bn_running_mean)
          bn.running_var.copy_(bn_running_var)

        else:
          #Number of biases
          num_biases = conv.bias.numel()

          #Load the weights
          conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])
          ptr = ptr + num_biases

          #reshape the loaded weights according to the dims of the model weights
          conv_biases = conv_biases.view_as(conv.bias.data)

          #Finally copy the data
          conv.bias.data.copy_(conv_biases)
        
        #Let us load the weights for the Convolutional layers
        num_weights = conv.weight.numel()

        #Do the same as above for weights
        conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])
        ptr = ptr + num_weights

        conv_weights = conv_weights.view_as(conv.weight.data)
        conv.weight.data.copy_(conv_weights)



"""### YOLO (Detection Layer)

The output of YOLO is a convolutional feature map that contains the bounding box attributes along the depth of the feature map.The attributes bounding boxes predicted by a cell are stacked one by one along each other. So, if you have to access the second bounding of cell at (5,6), then you will have to index it by map[5,6, (5+C): 2*(5+C)]. This form is very inconvenient for output processing such as thresholding by a object confidence, adding grid offsets to centers, applying anchors etc.

Since detections happen at three scales, the dimensions of the prediction maps will be different. Although the dimensions of the three feature maps are different, the output processing operations to be done on them are similar. It would be nice to have to do these operations on a single tensor, rather than three separate tensors.

To remedy these problems, we introduce the function `predict_transform`

### Testing the Network code
"""

model = Darknet('yolov3.cfg')
inp = get_test_input()
pred = model(inp, torch.cuda.is_available())
print(pred)
print(pred.shape)

"""The shape of this tensor is `1 x 22743 x 85`.
The first dimension is the batch size which is simply 1 because , we have used a single image.
For each image in a batch, we have a 22743 x 85 table. The row of each of this table represents a bounding box.(4 bbox attributes, 1 objectness score and 80 class scores)

At this point, our network has random weights and will not produce the correct output. We need to load a weight file for this purpose.

### Downloading and understanding the Pre-trained weights file

The official weights file is binary file that contains weights stored in a serial fashion

Weights are just stores as floats, with no information like, to which layer fo they belong to.Since, you are reading only floats , there;s no way to discriminate between which weight belongs to which layer

First, The weights belong to only two types of layers, either a batch norm layer or a convolutional layer.

The weights for these layers are stored exactly in the same order as they appear in the configuration file.
When the batch norm layer appears in a `convolutional` block, there are no biases. However, when there;s no batch norm layer, bias 'weights' have to read from the file

##### Loading the weights in `Darknet` object by calling the `load_weights` function
"""

model = Darknet('yolov3.cfg')
model.load_weights('yolov3.weights')